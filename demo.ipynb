{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Demo script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and examine the version of the Azure ML SDK. This demo was created using the 0.1.65 version of the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version:  0.1.68\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"SDK Version: \", azureml.core.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEFORE YOU BEGIN\n",
    "\n",
    "1. If you want this notebook to run from start to finish without needing to enter your credentials, make sure that you run the Connect to Azure command from the Data menu above to make your Azure Credentials available to the scripts running in this notebook. **If you see a device login prompt, this almost always means that you forgot to run the Data/Connect to Azure command.**\n",
    "\n",
    "This notebook takes ~25 minutes to run from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Workspaces\n",
    "\n",
    "The workspace is the top-level resource for the Azure Machine Learning service. It provides a centralized place to work with all the artifacts you create when using Azure Machine Learning Service.\n",
    "\n",
    "The workspace keeps a list of compute targets that can be used to train your model. It also keeps a history of the training runs, including logs, metrics, output, and a snapshot of your scripts. This information is used to determine which training run produces the best model.\n",
    "\n",
    "Models are registered with the workspace. A registered model and scoring scripts are used to create an image. The image can then be deployed into Azure Container Instances, Azure Kubernetes Service, or to a field-programmable gate array (FPGA) as a REST-based HTTP endpoint. It can also be deployed to an Azure IoT Edge device as a module.\n",
    "\n",
    "This would be a good time to graphically explore the ```TechSummit``` workspace using Visual Studio Code and the Azure ML extension. You can easily see the different kinds of resources that are there. Note that the Visual Studio Code Azure ML extension makes it easy to work with Azure ML workspaces directly from the editor that you're using to create your code. All of the operations that you can do in Visual Studio Code, including examining experiments etc. can be also done _programmatically_ in Azure Notebooks using the AML SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can interact with a workspace, we first need to acquire a reference to it. Create or open the ```TechSummit``` Azure ML workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_name = 'TechSummit'\n",
    "experiment_name = 'TechSummitDemo'\n",
    "simple_experiment_name = 'TechSummitDemoSimple'\n",
    "hyperdrive_experiment_name = 'TechSummitHyperDriveDemo'\n",
    "resource_group = 'juliademo'\n",
    "compute_name = 'nc6cluster'\n",
    "dataset_name = 'mnist'\n",
    "training_script_filename = 'tf_mnist.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TechSummit'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.get(name=workspace_name, \n",
    "                   subscription_id='15ae9cb6-95c1-483d-a0e3-b1a1a3b06324', \n",
    "                   resource_group=resource_group) \n",
    "ws.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can submit jobs to the cluster that we saw earlier, we need to retrieve a reference to it. Note that the ```nc6cluster``` has already been created earlier in this demo. The ```ComputeTarget.create``` method can be called to either create or retrieve a reference to an existing cluster; it will not return an error if the cluster already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = ws.compute_targets()[compute_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a simple experiment using our Azure Batch AI cluster\n",
    "\n",
    "Let's begin by creating a new experiment called ```TechSummitDemo``` in our workspace, and running the ```tf_mnist.py``` file using it. An **Experiment** constructs a reproducible execution environment for running your model training scripts. It does so by creating a **Docker container** for the execution environment for the experiment. The Docker container is saved to the Azure Container Registry in your **Workspace**, ensuring that you will always be able to come back to reproduce the results of this experiment in the future.\n",
    "\n",
    "Once the container is created, it must be downloaded onto each of the compute nodes on the cluster before the runs can begin. Note that this can take quite some time to complete the first time that you configure the Experiment. But once complete, you now can recreate the environment rapidly by re-running the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "simple_experiment = Experiment(ws, simple_experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run just one of the following two cells!\n",
    "\n",
    "Run the first one if you want to show the experiment running. This experiment takes ~90s to run on the cluster.\n",
    "\n",
    "Run the second cell if you want to pull data from a saved experimental run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TechSummitDemoSimple_1540352490318'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "mnist_data = ds.path(dataset_name)\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': mnist_data\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(source_directory='.',\n",
    "                       compute_target=cluster,\n",
    "                       entry_script=training_script_filename,\n",
    "                       script_params=script_params)\n",
    "\n",
    "run = simple_experiment.submit(estimator)\n",
    "run_id = run.id\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 'TechSummitDemoSimple_1540338450989'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine the ```run``` object, we can see a link to the Azure Portal that shows a summary of the run. If you don't see the Azure ML ```UserRun``` widget appearing below, you haven't installed the widget into your Jupyter Notebook instance. You will need to exit Jupyter, run the following two commands from the console that you started Jupyter from, and restart the notebook server.\n",
    "\n",
    "```\n",
    "jupyter nbextension install --py --user azureml.train.widgets\n",
    "jupyter nbextension enable azureml.train.widgets --user --py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf34645fcfd94fc387249f31f10334ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRun(widget_settings={'childWidgetDisplay': 'popup'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.core import Run\n",
    "from azureml.train.widgets import RunDetails\n",
    "\n",
    "RunDetails(Run(simple_experiment, run_id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HyperDrive to do hyper parameter optimization\n",
    "\n",
    "Now that we have a model, the next step is to tune it for hyperdrive parameters. Start by retrieving a reference to our HyperDrive experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ws.experiments()[experiment_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run just one of the following two cells!\n",
    "\n",
    "Run the first one if you want to show the experiment running. This experiment takes ~12 minutes to run on the cluster.\n",
    "\n",
    "Run the second cell if you want to pull data from a saved experimental run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveRunConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import loguniform, uniform\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "mnist_data = ds.path(dataset_name)\n",
    "\n",
    "script_params={\n",
    "    '--data-folder': mnist_data,\n",
    "}\n",
    "\n",
    "tf_estimator = TensorFlow(source_directory='.',\n",
    "                          compute_target=cluster,\n",
    "                          entry_script='tf_mnist.py',\n",
    "                          script_params=script_params)\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--learning-rate': loguniform(-15, -3)\n",
    "    }\n",
    ")\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.15, evaluation_interval=2)\n",
    "\n",
    "hyperdrive_run_config = HyperDriveRunConfig(estimator = tf_estimator, \n",
    "                                            hyperparameter_sampling = ps, \n",
    "                                            policy = early_termination_policy,\n",
    "                                            primary_metric_name = \"final_acc\",\n",
    "                                            primary_metric_goal = PrimaryMetricGoal.MAXIMIZE,\n",
    "                                            max_total_runs = 20,\n",
    "                                            max_concurrent_runs = 5)\n",
    "\n",
    "hd_run = experiment.submit(hyperdrive_run_config)\n",
    "hd_run_id = hd_run.id\n",
    "hd_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_run_id = 'TechSummitDemo_1540317335250'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71a05dde6964ac2bf4b794854be39b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDrive(widget_settings={'childWidgetDisplay': 'popup'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.core import Run\n",
    "from azureml.train.widgets import RunDetails\n",
    "\n",
    "RunDetails(Run(experiment, hd_run_id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the best model\n",
    "\n",
    "If you are pressed for time, you can just walk through the deployment code and skip to the next section, ```Testing our deployed model```.\n",
    "\n",
    "Let's deploy the best run in the Hyperdrive tuning session, deploy the model to ACI and make a prediction using it. \n",
    "Note that the line below waits for the Hyperdrive run to complete, and the ```get_best_run_by_primary_metric()``` method is currently running very slowly. Be patient.\n",
    "\n",
    "### Run just one of the following two cells!\n",
    "\n",
    "Run the first one if you want to pull the best run from the currently running experiment. Note that this will block the notebook until the run is complete (~12 minutes).\n",
    "\n",
    "Run the second cell if you want to pull data from a saved experimental run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_run.wait_for_completion()\n",
    "best_run = hd_run.get_best_run_by_primary_metric()\n",
    "best_run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "best_run_id = 'TechSummitDemo_1540317335250_2'\n",
    "best_run = Run(experiment, best_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model from the ```best_run``` with AML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist', model_path='outputs/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a file that contains the model scoring code. It loads the model that was generated from the earlier training session into Tensorflow, and implements a ```run``` method that invokes the model to make a prediction using data that is passed in from a Web Service wrapper that was created by the AML Service.\n",
    "\n",
    "The magic at the top of the cell simply takes the contents of the cell and writes it out to a local file called ```score.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global X, output, sess\n",
    "    tf.reset_default_graph()\n",
    "    model_root = Model.get_model_path('tf-dnn-mnist')\n",
    "    saver = tf.train.import_meta_graph(os.path.join(model_root, 'mnist-tf.model.meta'))\n",
    "    X = tf.get_default_graph().get_tensor_by_name(\"network/X:0\")\n",
    "    output = tf.get_default_graph().get_tensor_by_name(\"network/output/MatMul:0\")\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    saver.restore(sess, os.path.join(model_root, 'mnist-tf.model'))\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    out = output.eval(session = sess, feed_dict = {X: data})\n",
    "    y_hat = np.argmax(out, axis = 1)\n",
    "    return json.dumps(y_hat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```score.py``` file is run in an execution environment that contains the libraries that are required. The cell below constructs a new execution environment specification that includes both the Tensorflow runtime, as well as the ```numpy``` libary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "cd = CondaDependencies.create()\n",
    "cd.add_conda_package('numpy')\n",
    "cd.add_tensorflow_conda_package()\n",
    "cd.save_to_file(base_directory='./', conda_file_path='myenv.yml')\n",
    "\n",
    "print(cd.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime environment defined by the ```myenv.yml``` file is installed into a Docker container image that is created by the AML service. We also tell the container image what language runtime to use, and what file to run in the image (```score.py```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "imgconfig = ContainerImage.image_configuration(execution_script=\"score.py\", \n",
    "                                               runtime=\"python\", \n",
    "                                               conda_file=\"myenv.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That Docker container is serialized into an Azure Container Registry instance that is part of the resource group for the AML Workspace that we are using. There is also an Azure Container Instances service that is part of the resource group, and it is used to run the container that we provision below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'name':'mnist', 'framework': 'TensorFlow DNN'},\n",
    "                                               description='Tensorflow DNN on MNIST from HyperDrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the container onto ACI, and start it running. It takes ~9 minutes to complete this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "service = Webservice.deploy_from_model(workspace=ws,\n",
    "                                       name='tf-mnist-svc',\n",
    "                                       deployment_config=aciconfig,\n",
    "                                       models=[model],\n",
    "                                       image_config=imgconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the URI for the scoring service. This will be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.scoring_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our deployed model\n",
    "\n",
    "Assign the URI cached above in the notebook to a local variable. This is done for the cases where we don't have enough time to wait for the service to deploy, so we can just re-use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_uri = 'http://40.76.74.244:80/score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the training and the sample data for the mnist dataset. In the future, we should just keep the training dataset in a local directory in the project vs. taking the bandwidth to download the full training dataset. We're just using the test dataset to test our model's inferencing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def read_idx(filename):\n",
    "    with gzip.open(filename) as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "training_dataset = 't10k-images-idx3-ubyte.gz'\n",
    "training_labels = 't10k-labels-idx1-ubyte.gz'\n",
    "X_test = read_idx(training_dataset).reshape(10000, 784)\n",
    "y_test = read_idx(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a request to the web service that we just deployed at the ```service.scoring_uri```. The code below will render the digit that is being sent to our model, as well as showing what the original test label was, and the prediction that was returned from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST to url http://40.76.74.244:80/score\n",
      "label: 1\n",
      "prediction: \"[1]\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x282be4cc898>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "resp = requests.post(service_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service_uri)\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text)\n",
    "\n",
    "plt.imshow(X_test[random_index].reshape(28, 28), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
